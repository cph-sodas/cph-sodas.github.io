{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the SODAS Documentation","text":"<p>Find relevant documentation and information about SODAS here.</p>"},{"location":"server/","title":"Server","text":"<p>Welcome to the SODAS server.</p> <p>Before you start using the server, please make sure that you read all the three pages in this part of the documentation:</p> <ol> <li>How to access the server</li> <li>How to setup a project and your developer environment</li> <li>How to use the computational resources through Slurm</li> </ol>"},{"location":"server/access/","title":"Access","text":"<p>In order to access the server, you need to contact TBD with your basic information and SODAS affiliation, and they will setup up an account for you.</p>"},{"location":"server/access/#connecting-to-the-server","title":"Connecting to the server","text":"<p>First of all, you need to be connected to the university's VPN.</p> <p>You can connect to the server using SSH, either through the terminal or VSCode.</p>"},{"location":"server/access/#terminal","title":"Terminal","text":""},{"location":"server/access/#ssh","title":"SSH","text":"<pre><code>ssh -L &lt;local-port&gt;:localhost:&lt;remote-port&gt; &lt;username&gt;@sodashead01fl.unicph.domain\n</code></pre> <p>To connect locally to the server on port 8000 with the user abc123, do:</p> <pre><code>ssh -L 8000:localhost:8000 abc123@sodashead01fl.unicph.domain\n</code></pre>"},{"location":"server/access/#transferring-files","title":"Transferring files","text":""},{"location":"server/access/#transfer-from-local-to-server","title":"Transfer from local to server","text":"<pre><code>scp /path/to/file &lt;username&gt;@sodashead01fl.unicph.domain:/path/to/destination\n</code></pre> <p>To move a text file in documents to your H-drive which is mounted on the server by default, do:</p> <pre><code>scp /documents/test.txt abc123@sodashead01fl.unicph.doman:/ucph/hdir\n</code></pre>"},{"location":"server/access/#transfer-file-from-server-to-local","title":"Transfer file from server to local","text":"<pre><code>scp &lt;username&gt;@sodashead01fl.unicph.domain:/path/to/file /path/to/destination\n</code></pre> <p>To move a text file from the H-drive on the server to your local documents folder, do:</p> <pre><code>scp abc123@sodashead01fl.unicph.doman:/ucph/hdir/test.txt /documents\n</code></pre>"},{"location":"server/access/#vscode","title":"VSCode","text":"<p>TBD</p>"},{"location":"server/access/#management","title":"Management","text":"<p>Display unicph ID information</p> <pre><code>kuid &lt;username&gt;\n</code></pre> <p>Check user access</p> <pre><code>id &lt;username&gt; | tr \",\" \"\\n\" | grep srv-sodas\n</code></pre>"},{"location":"server/setup/","title":"Setup","text":"<p>The setup guide walks you through the process of setting up your developer environment on the SODAS server.</p>"},{"location":"server/setup/#projects","title":"Projects","text":""},{"location":"server/setup/#storage","title":"Storage","text":""},{"location":"server/setup/#shared-cache","title":"Shared cache","text":""},{"location":"server/setup/#configuration","title":"Configuration","text":""},{"location":"server/setup/#install-custom-software","title":"Install custom software","text":""},{"location":"server/setup/#uv","title":"uv","text":"<p>Install <code>uv</code>:</p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <p>Add an alias to your shell to easily activate virtual environments:</p> <pre><code>echo \"alias activate=\\\"source ./.venv/bin/activate\\\"\" &gt;&gt; ~/.bashrc\n</code></pre> <p>Tip</p> <p>Remeber to source your <code>.bashrc</code> after adding the alias. You can do this by <pre><code>source ~/.bashrc\n</code></pre></p> <pre><code>uv venv\nactivate\n</code></pre> <pre><code>uv add matplotlib\n</code></pre>"},{"location":"server/setup/#jupyterlab","title":"JupyterLab","text":"<p>Warning</p> <p>The following steps do not fully work, since <code>uv</code> havn't added --include-deps flag yet</p> <p>To install jupyterlab system-wide, we can run:</p> <pre><code>uv tool install jupyterlab\n</code></pre> <pre><code>echo \"alias jlab=\\\"jupyter-lab --port=8880 --ip=10.84.10.216 --no-browser\\\"\" &gt;&gt; ~/.bashrc\necho \"alias jadd=\\\"python -m ipykernel install --user --display-name \\${PWD} --name \\${PWD##*/}\\\"\" &gt;&gt; ~/.bashrc\n</code></pre>"},{"location":"server/setup/#zsh","title":"zsh","text":"<p>In this step of the guide, we will take you throw setting up zsh as your default shell, and installing oh-my-zsh</p> <p>Warning</p> <p>After setting up zsh this way installing oh-my-zsh throws some errors, probably not the right way. Maybe try installing zsh from source</p> <p>Make sure the necessary directories exist:</p> <pre><code>mkdir -p ~/.local/bin/\n</code></pre> <p>create a symlink to the zsh already installed</p> <pre><code>ln -s /opt/softare/zsh/5.9/bin/zsh ~/.local/bin/zsh\n</code></pre>"},{"location":"server/setup/#environment-variables","title":"Environment Variables","text":""},{"location":"server/setup/#docker","title":"Docker","text":""},{"location":"server/setup/#python-dependency-management","title":"Python dependency management","text":""},{"location":"server/setup/#utilities","title":"Utilities","text":""},{"location":"server/setup/#shell","title":"Shell","text":""},{"location":"server/setup/#github","title":"Github","text":""},{"location":"server/setup/#generate-ssh-key","title":"Generate SSH key","text":"<p>Follow the guide at GitHub.</p>"},{"location":"server/setup/#ssh-config","title":"SSH Config","text":"<p>Create a config file at <code>~/.ssh/config</code> and add the following lines:</p> <pre><code>Host github.com\n    HostName ssh.github.com\n    PreferredAuthentications publickey\n    IdentityFile ~/.ssh/id_ed25519\n    Port 443\n</code></pre>"},{"location":"server/usage/","title":"Usage","text":"<p>All usage of computer resources is manged through the Slurm<sup>1</sup> Workload Manager.</p>"},{"location":"server/usage/#modules","title":"Modules","text":""},{"location":"server/usage/#slurm","title":"Slurm","text":"<p>Slurm is a job scheduler and resource manager for the compute resources available.</p>"},{"location":"server/usage/#status","title":"Status","text":"<p>To see the attached resources, you can run the following command:</p> <pre><code>sinfo -N -l\n</code></pre> <p>To see the jobs that are currently running, you can run the following command:</p> <pre><code>squeue\n</code></pre> <p>for a specific user:</p> <pre><code>squeue -u &lt;username&gt;\n</code></pre>"},{"location":"server/usage/#submitting-jobs","title":"Submitting jobs","text":""},{"location":"server/usage/#batch-jobs","title":"Batch jobs","text":"<pre><code>#!/bin/bash\n#SBATCH --chdir=/projects/main_compute-AUDIT/people/abc123\n#SBATCH --job-name alphafoldtestjobname\n#SBATCH --mem=50G\n#SBATCH --ntasks=1                # 1 tasks\n#SBATCH --cpus-per-task=1         # number of cores per task\n#SBATCH --nodes=1                 # number of nodes\n#SBATCH --mail-type=begin         # send email when job begins\n#SBATCH --mail-type=end           # send email when job ends\n#SBATCH --mail-user=abc123@ku.dk\n#SBATCH --gres=gpu:1\n\n# Load modules\nmodule load miniconda/4.10.4\nconda activate alphafold\n\n# Run the script\ncd /projects/main_compute-AUDIT/data/alphafold\nbash run_alphafold.sh -d /projects/testproject1/data/genetic_databases/ -o /projects/testproject1/people/btj820/ -m model_1 -f example/query.fasta -t 2020-05-14\n</code></pre> <pre><code>sbatch &lt;script&gt;.sh\n</code></pre> <ol> <li><code>--mail-type=type</code> notify on state change: <code>BEGIN</code>, <code>END</code>, <code>FAIL</code> or <code>ALL</code></li> </ol>"},{"location":"server/usage/#interactive-jobs","title":"Interactive jobs","text":"<p>To start a simple interactive shell with 2 CPU cores, 5GB ram, 1 v100 GPU you can run the following command:</p> <p>Tip</p> <p>If copy pasting doesn't work for the multi line code snippets, try switching between selecting the text and using the copy button in the top right corner</p> <pre><code>srun -w sodasgpun01fl --partition=gpuqueue \\ #(1)!\n    --ntasks-per-node=2 \\ #(2)!\n    --mem=5GB \\ #(3)!\n    --gres=gpu:v100:1 \\ #(4)!\n    --pty /bin/bash -i #(5)!\n</code></pre> <ol> <li>Standard node and partition configuration</li> <li>Number of CPU cores</li> <li>Amount of memory (RAM)</li> <li>Number of GPUs</li> <li>Run task in pseudo terminal</li> </ol>"},{"location":"server/usage/#jupyter-notebook","title":"Jupyter Notebook","text":"<p>To start a Jupyter Notebook, you need to first allocate resources on the server:</p> <pre><code>srun -w sodasgpun01fl --partition=gpuqueue \\\n  --ntasks-per-node=2 \\\n  --mem=5GB \\\n  --pty /bin/bash -i\n</code></pre> <p>Then, load the Jupyter Notebook module:</p> <pre><code>module load jupyter-notebook/6.5.4\n</code></pre> <p>Now, you can start the notebook server:</p> <pre><code>jupyter notebook --port=8880 --ip=10.84.10.216 --no-browser\n</code></pre> <p>Then copy the generated link and paste it in your local computer's browsers.</p> <p>I.e: <code>http://10.84.10.216:8800/lab?token=abcd1234...</code></p>"},{"location":"server/usage/#persistent-sessions","title":"Persistent sessions","text":"<p>Use tmux to create and manage persistent sessions on the server.</p> <p>Start a new tmux session</p> <pre><code>tmux new -s &lt;session-name&gt;\n</code></pre> <p>List tmux sessions</p> <pre><code>tmux ls\n</code></pre> <p>Attach tmux session</p> <pre><code>tmux a -t &lt;session name&gt;\n</code></pre> <p>Detach (when you are inside) the session from tmux, leaving everything running in the background</p> <p>Ctrl+B D</p>"},{"location":"server/usage/#resources","title":"Resources","text":"<p>The UCPH guide to HPC systems</p> <p>Five part  video series  introducing Slurm</p> <p>The official slurm cheatsheet</p> <p>TMUX cheatsheet</p> <ol> <li> <p>Simple Linux Utility for Resource Management.\u00a0\u21a9</p> </li> </ol>"}]}